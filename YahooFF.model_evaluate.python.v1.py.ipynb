{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Model Evaluation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Choose, justify and apply a model performance indicator (e.g. F1 score, true positive rate, within cluster sum of squared error, \u2026) to assess your model and justify the choice of an algorithm\n\n- Implement your algorithm in at least one deep learning and at least one non-deep learning algorithm, compare and document model performance\n\n- Apply at least one additional iteration in the process model involving at least the feature creation task and record impact on model performance (e.g. data normalizing, PCA, \u2026)\n\n- Depending on the algorithm class and data set size you might choose specific technologies / frameworks to solve your problem. Please document all your decisions in the ADD (Architectural Decisions Document).\n<br><font color=blue></font>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load cleaned data\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "preppedDataDF = spark.read.parquet('preppedDataDF.parquet')\npreppedDataDF.createOrReplaceTempView(\"preppedDataDF\")"
        }, 
        {
            "source": "### Third training iteration after adding new features with function and loop\n\nNote version files might be a bit different as I had worked on model def, training and eval all in one notebook to start but to finalize the project now, I'm moving each piece to its own notebook.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Load trained models and evaluate", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "I use the confusion matrix as a measure of performance for these models.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Model 1: Logistic Regression (Classification)\n### Supervised machine learning\n#### Classification of tiers of players", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import LogisticRegressionModel\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics"
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lrModel = LogisticRegressionModel.load(\"lrModel_trained\")"
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result = lrModel.transform(test)\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nmetrics = MulticlassMetrics(predictionAndLabels.rdd)"
        }, 
        {
            "execution_count": 42, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/spark21master/spark/python/pyspark/mllib/evaluation.py:237: UserWarning: Deprecated in 2.0.0. Use accuracy.\n  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Precision: 0.7849223946784922, Recall: 0.7849223946784922\nF-Score: 0.7849223946784922\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/spark21master/spark/python/pyspark/mllib/evaluation.py:249: UserWarning: Deprecated in 2.0.0. Use accuracy.\n  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n/usr/local/src/spark21master/spark/python/pyspark/mllib/evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
                }
            ], 
            "source": "# pyspark's implementation is bad (precision, recall and metrics are all the same)\nprint('Precision: {}, Recall: {}'.format(metrics.precision(), metrics.recall()))\nprint('F-Score: {}'.format(metrics.fMeasure()))"
        }, 
        {
            "execution_count": 43, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 43, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DenseMatrix(4, 4, [43.0, 15.0, 0.0, 1.0, 8.0, 18.0, 8.0, 1.0, 1.0, 7.0, 6.0, 0.0, 1.0, 14.0, 41.0, 287.0], 0)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "metrics.confusionMatrix()"
        }, 
        {
            "source": "After tuning the hyper-parameters on this model, the preformance has increased.  \nPreviously, all of the predictions are of the same class.  It appears that most of the player and team stats are quite similar so there is no clear linaer boundary using logistic regression.\nHowever, using the tuned model produced much better results.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Optimal hyper-parameters used:\n"
                }, 
                {
                    "ename": "AttributeError", 
                    "evalue": "'LogisticRegressionModel' object has no attribute 'stages'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-56-d5d289f56dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimal hyper-parameters used:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlrModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegressionModel' object has no attribute 'stages'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "print('Optimal hyper-parameters used:')\nlrModel.stages"
        }, 
        {
            "source": "## Model 2: MultilayerPerceptronClassifier (MLP) (Classification)\n### More primitive deep learning\n#### Classification of tiers of players", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import MultilayerPerceptronClassificationModel"
        }, 
        {
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "MLPModel = MultilayerPerceptronClassificationModel.load(\"MLPModel_trained\")"
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result2 = MLPModel.transform(test)\npredictionAndLabels2 = result2.select(\"prediction\", \"label\")\nevaluator2 = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nmetrics2 = MulticlassMetrics(predictionAndLabels2.rdd)"
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 59, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DenseMatrix(4, 4, [6.0, 5.0, 7.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 47.0, 49.0, 48.0, 287.0], 0)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "metrics2.confusionMatrix()"
        }, 
        {
            "source": "MLP performs substantially worse than logistic regression in this example.  Most of the predictions are in class 3 where most of the players (the ones not ranked in the top 15) are.  There are a few predictions for the top class.  The accuracy for that class is not great though: 6 / 20.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Model 3: Neural Net (Classification)\n### Deep learning\n#### Classification of tiers of players", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define the model by subclassing Module\nclass Net(nn.Module):\n\n    def __init__(self, D_in, H, D_out):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear modules and assign them as\n        member variables.\n\n        D_in: input dimension\n        H: dimension of hidden layer\n        D_out: output dimension\n        \"\"\"\n        super(Net, self).__init__()\n        # definte layers here.  can be re-used\n        self.layer_1 = nn.Linear(D_in, H, bias=True)\n        self.relu = nn.ReLU()\n        self.layer_2 = nn.Linear(H, H, bias=True)\n        self.output_layer = nn.Linear(H, D_out, bias=True)\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Variable of input data and we must \n        return a Variable of output data. We can use Modules defined in the \n        constructor as well as arbitrary operators on Variables.\n        \"\"\"\n        out = self.layer_1(x)\n        out = self.relu(out)\n        out = self.layer_2(out)\n        out = self.relu(out)\n        out = self.output_layer(out)\n        return out"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = torch.load(\"torch_nn\")"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.load_state_dict(torch.load(\"torch_nn_trained.pt\"))"
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from torch.autograd import Variable\nimport collections"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# unfortunately, a multilabelconfusionmeter isn't available in pytorch yet\n# https://discuss.pytorch.org/t/multilabelconfusionmeter-in-pytorch/12803\n# so had to write one myself\n\ndef multilabelconfusionmeter(test_dataset, model):\n    '''\n    :param test_dataset: test data set that was split (or use in-sample)\n    :param model: the instance of my model class that subclasses nn.Module\n    '''\n    # returns a matrix of predicted values for rows, actual values for columns and the number of hits in the columns. \n    total_test_data = test_dataset.count()\n    batch_x_test = torch.tensor(test_dataset.toPandas()['features'])\n    batch_y_test = torch.tensor(test_dataset.toPandas()['label']).long()    \n    ff_features = Variable(torch.FloatTensor(batch_x_test))\n    labels = Variable(torch.LongTensor(batch_y_test))\n    num_labels = len(labels.unique())\n    outputs = model(ff_features)\n    _, predicted = torch.max(outputs.data, 1)\n    \n    confusion_matrix = collections.defaultdict(dict)   \n    # initialize the results dictionary\n    for i in range(num_labels):\n        for j in range(num_labels):        \n            confusion_matrix[i][j] = 0\n            \n    # loop and fill in matrix\n    for a, p in zip(labels, predicted):\n        # labels are the actual value    \n        confusion_matrix[p.item()][a.item()] += 1\n    return confusion_matrix"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 34, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "defaultdict(dict,\n            {0: {0: 48, 1: 3, 2: 0, 3: 1},\n             1: {0: 5, 1: 42, 2: 8, 3: 1},\n             2: {0: 0, 1: 8, 2: 42, 3: 6},\n             3: {0: 0, 1: 1, 2: 5, 3: 281}})"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "mcm = multilabelconfusionmeter(test, model)\nmcm"
        }, 
        {
            "source": "The neural net fares much better at prediction.  The highest and lowest classes are quite good.  The middle classes are smaller in size and the NN gets those correct much more often than the original model and feature specifications.  The neural net compares favorably to the other models.  In fact, with the additional features created in feat_eng.v4, this iteration is better than the original performance of the model as well.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Class 0 accuracy: 92.31%\nClass 1 accuracy: 75.00%\nClass 2 accuracy: 75.00%\nClass 3 accuracy: 97.91%\n"
                }
            ], 
            "source": "for x in [0,1,2,3]:\n    print('Class {} accuracy: {:.2f}%'.format(\n        x, mcm[x][x] / sum(list(mcm[x].values()))* 100))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}