{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Model Training", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Choose, justify and apply a model performance indicator (e.g. F1 score, true positive rate, within cluster sum of squared error, \u2026) to assess your model and justify the choice of an algorithm\n\n- Implement your algorithm in at least one deep learning and at least one non-deep learning algorithm, compare and document model performance\n\n- Apply at least one additional iteration in the process model involving at least the feature creation task and record impact on model performance (e.g. data normalizing, PCA, \u2026)\n\n- Depending on the algorithm class and data set size you might choose specific technologies / frameworks to solve your problem. Please document all your decisions in the ADD (Architectural Decisions Document).\n<br><font color=blue></font>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# load cleaned data\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "preppedDataDF = spark.read.parquet('preppedDataDF.parquet')\npreppedDataDF.createOrReplaceTempView(\"preppedDataDF\")"
        }, 
        {
            "source": "### Third training iteration after adding new features with function and loop\n\nNote version files might be a bit different as I had worked on model def, training and eval all in one notebook to start but to finalize the project now, I'm moving each piece to its own notebook.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Get training and test datasets", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from systemml import MLContext, dml\nml = MLContext(spark)"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# base the classification on previous years attempts for the players position\n# create a label and class to regress against.\nsdf2 = spark.sql('''\nSELECT player_tier AS label, features\nFROM preppedDataDF\n''')\n#SELECT cast(player_att_norm as float) AS class, player_tier AS label, features\nsdf2.createOrReplaceTempView(\"sdf2\")"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Split the data into train and test\nsplits = sdf2.randomSplit([0.6, 0.4], seed=1234)\ntrain = splits[0]\ntest = splits[1]"
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_inputs = len(train.toPandas()['features'][0])"
        }, 
        {
            "source": "## Model 1: Logistic Regression (Classification)\n### Supervised machine learning\n#### Classification of tiers of players", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics"
        }, 
        {
            "execution_count": 160, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lr = LogisticRegression.load(\"logistic_regression\")"
        }, 
        {
            "source": "**Hyper-parameter tuning**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 161, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# for hyper parameters tuning\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
        }, 
        {
            "execution_count": 162, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.5, 0.3, 0.1, 0.01]).build()"
        }, 
        {
            "execution_count": 163, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "crossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=MulticlassClassificationEvaluator(),\n                          numFolds=3)"
        }, 
        {
            "execution_count": 164, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Run cross-validation, and choose the best set of parameters.\nlr_cvModel = crossval.fit(train)"
        }, 
        {
            "execution_count": 165, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result = lr_cvModel.transform(test)\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nmetrics = MulticlassMetrics(predictionAndLabels.rdd)"
        }, 
        {
            "execution_count": 166, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 166, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.7849223946784922"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "evaluator.evaluate(predictionAndLabels)"
        }, 
        {
            "execution_count": 173, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 173, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "DenseVector([-0.3803, -0.3984, -0.5242, 1.3029])"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "lr_cvModel.bestModel.interceptVector"
        }, 
        {
            "execution_count": 119, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!rm -rf lrModel_trained\nlr_cvModel.bestModel.save(\"lrModel_trained\")"
        }, 
        {
            "source": "## Model 2: MultilayerPerceptronClassifier (MLP) (Classification)\n### More primitive deep learning\n#### Classification of tiers of players", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import MultilayerPerceptronClassifier"
        }, 
        {
            "execution_count": 155, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "MLP_trainer = MultilayerPerceptronClassifier.load(\"MLP_trainer\")"
        }, 
        {
            "execution_count": 156, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "# train the model\nMLPModel = MLP_trainer.fit(train)"
        }, 
        {
            "execution_count": 157, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "# compute accuracy on the test set\nresult2 = MLPModel.transform(test)\npredictionAndLabels2 = result2.select(\"prediction\", \"label\")\nevaluator2 = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nmetrics2 = MulticlassMetrics(predictionAndLabels2.rdd)"
        }, 
        {
            "execution_count": 158, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "execution_count": 158, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.6496674057649667"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "evaluator2.evaluate(predictionAndLabels2)"
        }, 
        {
            "source": "From training this MLP to predict player rank from just number of plays, we find that it is 65% accurate.  This is not a great result given that most players would be in tier 3.  On various runs, MLP has about the same performance as logistic regression.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 159, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!rm -rf \"MLPModel_trained\"\nMLPModel.save(\"MLPModel_trained\")"
        }, 
        {
            "source": "## Model 3: Neural Net (Classification)\n### Deep learning\n#### Classification of tiers of players", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F"
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import sys\nimport os\nimport imp"
        }, 
        {
            "source": "I tried to load the pytorch model as is.  But there are issues with that, it can't read the class.  A slightly cleaner solution is to load the class explicitly.\n\nSee: https://discuss.pytorch.org/t/error-loading-saved-model/8371/4\n\nI tried to hack around it by using imp to load the class in the module manually (the periods in the file naming convention prevent a simple 'from module import Net').\nThat too didn't work so I'm defining the Net class again here in this file.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 57, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'/gpfs/global_fs01/sym_shared/YPProdSpark/user/s6b7-e822a2b9f546a1-2b5348f6e911/notebook/work'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "os.getcwd()"
        }, 
        {
            "execution_count": 51, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sys.path.append(os.getcwd())"
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [
                {
                    "ename": "FileNotFoundError", 
                    "evalue": "[Errno 2] No such file or directory: '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s6b7-e822a2b9f546a1-2b5348f6e911/notebook/work/YahooFF.model_def.python.v4.py'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-58-9ccb5a3621b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Net'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/gpfs/global_fs01/sym_shared/YPProdSpark/user/s6b7-e822a2b9f546a1-2b5348f6e911/notebook/work/YahooFF.model_def.python.v4.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mload_source\u001b[0;34m(name, pathname, file)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;31m# To allow reloading to potentially work, use a non-hacked loader which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# won't rely on a now-closed file object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(spec)\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n", 
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s6b7-e822a2b9f546a1-2b5348f6e911/notebook/work/YahooFF.model_def.python.v4.py'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "Net = imp.load_source('Net', '/gpfs/global_fs01/sym_shared/YPProdSpark/user/s6b7-e822a2b9f546a1-2b5348f6e911/notebook/work/YahooFF.model_def.python.v4.py')"
        }, 
        {
            "source": "Loading model explicitly", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define the model by subclassing Module\nclass Net(nn.Module):\n\n    def __init__(self, D_in, H, D_out):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear modules and assign them as\n        member variables.\n\n        D_in: input dimension\n        H: dimension of hidden layer\n        D_out: output dimension\n        \"\"\"\n        super(Net, self).__init__()\n        # definte layers here.  can be re-used\n        self.layer_1 = nn.Linear(D_in, H, bias=True)\n        self.relu = nn.ReLU()\n        self.layer_2 = nn.Linear(H, H, bias=True)\n        self.output_layer = nn.Linear(H, D_out, bias=True)\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Variable of input data and we must \n        return a Variable of output data. We can use Modules defined in the \n        constructor as well as arbitrary operators on Variables.\n        \"\"\"\n        out = self.layer_1(x)\n        out = self.relu(out)\n        out = self.layer_2(out)\n        out = self.relu(out)\n        out = self.output_layer(out)\n        return out\n"
        }, 
        {
            "execution_count": 63, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = torch.load(\"torch_nn\")"
        }, 
        {
            "execution_count": 64, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def one_hot_encode(target_rows, target_cols, y_tensor):\n    '''\n    :param target_rows: row dimension\n    :param target_cols: column dimension\n    :param y_tensor: the y which is a tensor of integers that you want 1 hot encoded (column dim must be correct)\n    '''\n    # initialize a tensor with the desired dimensions\n    y_onehot = torch.LongTensor(target_rows, target_cols)\n    # loop through.  make the whole row zero and then set the index indicated by each value in y_tensor to 1\n    for y_row, y1s_row in zip(y.view(-1,1),y_onehot):\n        y1s_row.zero_()    \n        y1s_row.scatter_(-1, y_row.long(), 1)\n    return y_onehot"
        }, 
        {
            "execution_count": 65, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# you have to one hot encode y so you get a tensor of size 827x4\ny_onehot = one_hot_encode(827, 4, y)"
        }, 
        {
            "execution_count": 66, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 66, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "tensor([[1, 0, 0, 0],\n        [1, 0, 0, 0],\n        [1, 0, 0, 0],\n        ...,\n        [0, 0, 0, 1],\n        [0, 0, 0, 1],\n        [0, 0, 0, 1]])"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "y_onehot"
        }, 
        {
            "execution_count": 67, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# loss_fn = torch.nn.MSELoss(reduction='sum')\nloss_fn = nn.CrossEntropyLoss()"
        }, 
        {
            "execution_count": 68, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "learning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
        }, 
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "epoch: 1, mini-batch: 5000, loss: 0.6576940417289734\nepoch: 1, mini-batch: 10000, loss: 0.3151319921016693\nepoch: 2, mini-batch: 5000, loss: 0.24604155123233795\nepoch: 2, mini-batch: 10000, loss: 0.22356271743774414\nFinished Training\n"
                }
            ], 
            "source": "# Use the optim package to define an Optimizer that will update the weights of\n# the model for us. Here we will use Adam; the optim package contains many other\n# optimization algoriths. The first argument to the Adam constructor tells the\n# optimizer which Tensors it should update.\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nmodel.train()  # This corrects for the differences in dropout, batch normalization during training and testing.\nfor epoch in range(2):\n    for t in range(10000):\n        # Forward pass: compute predicted y by passing x to the model.\n        y_pred = model(x)\n\n        # Compute and print loss.\n        # if using crossentropy loss, it expects class indices.  if MSELoss, onehot\n        if type(loss_fn) is torch.nn.modules.loss.CrossEntropyLoss:\n            loss = loss_fn(y_pred, y.long())\n        else:\n            loss = loss_fn(y_pred, y_onehot.float())\n        if t % 5000 == 4999:  # print every 5000 mini-batches\n            print('epoch: {}, mini-batch: {}, loss: {}'.format(epoch+1, t+1, loss.item()))\n\n        # Before the backward pass, use the optimizer object to zero all of the\n        # gradients for the variables it will update (which are the learnable\n        # weights of the model). This is because by default, gradients are\n        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n        # is called. Checkout docs of torch.autograd.backward for more details.\n        optimizer.zero_grad()\n\n        # Backward pass: compute gradient of the loss with respect to model\n        # parameters\n        loss.backward()\n\n        # Calling the step function on an Optimizer makes an update to its\n        # parameters\n        optimizer.step()\n        \nprint('Finished Training')"
        }, 
        {
            "execution_count": 70, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 70, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Net(\n  (layer_1): Linear(in_features=24, out_features=20, bias=True)\n  (relu): ReLU()\n  (layer_2): Linear(in_features=20, out_features=20, bias=True)\n  (output_layer): Linear(in_features=20, out_features=4, bias=True)\n)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.eval()  # This corrects for the differences in dropout, batch normalization during training and testing."
        }, 
        {
            "execution_count": 74, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "torch.save(model.state_dict(), \"torch_nn_trained.pt\")"
        }, 
        {
            "execution_count": 77, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from torch.autograd import Variable"
        }, 
        {
            "execution_count": 78, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Test the Model\ncorrect = 0\ntotal = 0\ntotal_test_data = test.count()\nbatch_x_test = torch.tensor(test.toPandas()['features'])\nbatch_y_test = torch.tensor(test.toPandas()['label']).long()\n# batch_y_test = one_hot_encode(total_test_data, 4, batch_y_test)\nff_features = Variable(torch.FloatTensor(batch_x_test))\nlabels = Variable(torch.LongTensor(batch_y_test))\noutputs = model(ff_features)\n_, predicted = torch.max(outputs.data, 1)\ntotal += labels.size(0)\ncorrect += (predicted == labels).sum().item()"
        }, 
        {
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Got 91.57% : 413 out of 451\n"
                }
            ], 
            "source": "print('Got {:.2f}% : {} out of {}'.format(correct/total*100,correct,total))"
        }, 
        {
            "source": "This is a much better result than the MLP or logistic regression and a better result than the first version of the neural network.\n\nThe difference this time is the additional features using the z-score for the player's stats relative to the other players at his same position for the same season.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}